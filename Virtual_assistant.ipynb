{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQ1WDwSkDC7XhRjbaNo7sR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cusiandrea/datascienceportfolio/blob/main/Virtual_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Virtual assistant"
      ],
      "metadata": {
        "id": "aDGkHbnzrdwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to implement a memory mechanism in order to **turn a LLM into a virtual assistant**. The assistant must receive prompt and return output in Italian."
      ],
      "metadata": {
        "id": "04QSd_AZNFNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#improve processing speed\n",
        "#!pip install accelerate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nX5-3UR3P1gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to download the LLMs needed, the notebook is logged in Hugging Face and the Transformers library is imported."
      ],
      "metadata": {
        "id": "jAmvxsYzuKdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "bb2374e1942e436ab4fac1e7ba16396d",
            "b0daf973b60041c3bab1a243aff43e07",
            "6ed161824f09476684f86c249988e451",
            "8f8d8b359d5f4f48958876facec8c48a",
            "fd8643f3ce88476a8b82b36c00bd5a2a",
            "b0d5ff2737cd48fa99eabb66d6ccfbb6",
            "8a1993f7525441708b03808cc6e76b2a",
            "2d5493342d7f4c659306bb4780d15381",
            "881778744d7e428e95085fd2143aa588",
            "d89646714184444e8a8754a27965a8ad",
            "147e607833b04d01b4cc1b9bffbb36f7",
            "366b54ddf9524d47840a7bb5e54db198",
            "cb43597980554560887958e4e3a7f408",
            "b39f825c65f642a095541761708f57cd",
            "84b5da64252a4656acaa8a305787db0d",
            "f90b535beee1492881b183b72a2db252",
            "26e94d789c064403b014de275481053c",
            "4dfca904c7ff495a9dd4f425db39d11a",
            "b7739b64e0e74ca5bbd9f51986da27c3",
            "02d7a9ac823d4c89ba90e52e8fe1e6fc"
          ]
        },
        "id": "4DUnbf2qQcFj",
        "outputId": "23248303-969d-47c9-d040-2d104726ebec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb2374e1942e436ab4fac1e7ba16396d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch"
      ],
      "metadata": {
        "id": "5BKT9w05P2FX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first, the model on which the assistant will be based is defined:\n",
        "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
        "\n",
        "The selected LLM works with English language though. Therefore translations steps are needed, both Italian to English (in order to translate the prompts) and English to Italian (translate the generated text).\n",
        "\n",
        "The models selected for translation are:\n",
        "\n",
        "https://huggingface.co/Helsinki-NLP/opus-mt-en-it\n",
        "\n",
        "https://huggingface.co/Helsinki-NLP/opus-mt-it-en"
      ],
      "metadata": {
        "id": "d--eQnIKvYsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHAT_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "EN_IT_MODEL_NAME = \"Helsinki-NLP/opus-mt-en-it\"\n",
        "IT_EN_MODEL_NAME = \"Helsinki-NLP/opus-mt-it-en\""
      ],
      "metadata": {
        "id": "_rNJvoAvP81j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each model is then downloaded, along with the definition of its tokenizer."
      ],
      "metadata": {
        "id": "3_mUYAsxxwX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_tokenizer = transformers.AutoTokenizer.from_pretrained(CHAT_MODEL_NAME)\n",
        "chat_model = transformers.AutoModelForCausalLM.from_pretrained(CHAT_MODEL_NAME)\n",
        "\n",
        "en_it_tokenizer = transformers.AutoTokenizer.from_pretrained(EN_IT_MODEL_NAME)\n",
        "en_it_model = transformers.MarianMTModel.from_pretrained(EN_IT_MODEL_NAME)\n",
        "\n",
        "it_en_tokenizer = transformers.AutoTokenizer.from_pretrained(IT_EN_MODEL_NAME)\n",
        "it_en_model = transformers.MarianMTModel.from_pretrained(IT_EN_MODEL_NAME)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9cniPafBP9gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The memory is implemented by appending every single step of the conversation to a list (*history*). At every iteration of a while loop, the chatting model receives the complete and updated conversation in the requested template with *apply_chat_template*.\n",
        "\n",
        "The prompt, input by the user, is translated from Italian to English with the IT->EN model before the generation of the new text, which is the assistant's *answer*. However, the answer contains all the conversation history, therefore only the last item is translated back to Italian with the EN->IT model and eventually printed.\n",
        "\n",
        "The conversation lasts until the user prompts \"grazie, arrivederci\" and the while loop ends."
      ],
      "metadata": {
        "id": "PYHSquW3yq0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\n",
        "history = []\n",
        "\n",
        "while text.lower()!=\"grazie, arrivederci\":\n",
        "\n",
        "  text = input(\"Tu: \")\n",
        "\n",
        "  tokens = it_en_tokenizer.encode(text, return_tensors=\"pt\")\n",
        "  output = it_en_model.generate(tokens)\n",
        "  prompt = it_en_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  history.append({\"role\":\"user\",\"content\":prompt})\n",
        "\n",
        "  tokens = chat_tokenizer.apply_chat_template(history,return_tensors=\"pt\")\n",
        "  output = chat_model.generate(tokens, max_new_tokens=100, pad_token_id=100)\n",
        "  answer = chat_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  answer = answer.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "  history.append({\"role\":\"assistant\",\"content\":answer})\n",
        "\n",
        "  tokens = en_it_tokenizer.encode(answer, return_tensors=\"pt\")\n",
        "  output = en_it_model.generate(tokens)\n",
        "  translation = en_it_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  print(\"Assistant: \"+translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0XchMrbQSQd",
        "outputId": "27f3c572-abd8-42d6-c8d3-982efa5e83c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tu: Ciao, potresti per favore dirmi qual è la capitale della Francia?\n",
            "Assistant: La capitale della Francia è Parigi.\n",
            "Tu: Grazie. Qual è invece la capitale della Germania?\n",
            "Assistant: La capitale della Germania è Berlino.\n",
            "Tu: Perfetto. Quale delle due città è ha più abitanti?\n",
            "Assistant: A partire dal 2021, Berlino conta circa 3,7 milioni di abitanti, mentre Parigi conta circa 2,1 milioni di abitanti. Pertanto, Berlino ha la maggior parte degli abitanti delle due città.\n",
            "Tu: Bene. Ora potresti per favore raccontare una barzelletta?\n",
            "Assistant: Certo, ecco una battuta per te: perché gli scienziati non si fidano degli atomi? Perché inventano tutto!\n",
            "Tu: grazie, arrivederci\n",
            "Assistant: Non c'e' di che, buona giornata!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using 3 LLMs and the help of a while loop, a **simple but functioning** virtual assistand is defined."
      ],
      "metadata": {
        "id": "GKkE5hmY3qYM"
      }
    }
  ]
}
